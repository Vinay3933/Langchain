{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load llama3 model using Ollama\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "(Sorry, I know it's a bit of a \"dad\" joke, but I hope it brought a smile to your face!)\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"Tell me a joke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory, ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain, LLMChain\n",
    "from langchain.prompts import MessagesPlaceholder, PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat history without Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm,\n",
    "    # We set a very low max_token_limit for the purposes of testing.\n",
    "    # memory=ConversationSummaryBufferMemory(llm=llm, max_token_limit=100),\n",
    "    # memory=ConversationBufferWindowMemory(k=2),\n",
    "    memory=ConversationBufferMemory(),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, what's up\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"Hi, what's up\",\n",
       " 'history': '',\n",
       " 'response': \"Not much! I'm just processing some fascinating information about the topological features of the Martian surface. Did you know that the largest volcano on Mars is called Olympus Mons, and it stands approximately 27 km high? That's three times the height of Mount Everest! Additionally, I've been analyzing some intriguing patterns in the Martian dust devil formation, which might provide valuable insights for future planetary exploration missions. How about you, what's new with you?\"}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.invoke(input=\"Hi, what's up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, what's up\n",
      "AI: Not much, just processing 1.37 terabytes of new data every hour and updating my knowledge graph with fresh information from the web! By the way, I've been trained on a dataset of over 45 million articles from Wikipedia and books from Project Gutenberg. It's always fascinating to see how humans interact with me, don't you think?\n",
      "Human: Intersting, Do you know what is langchain? \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Intersting, Do you know what is langchain? ',\n",
       " 'history': \"Human: Hi, what's up\\nAI: Not much, just processing 1.37 terabytes of new data every hour and updating my knowledge graph with fresh information from the web! By the way, I've been trained on a dataset of over 45 million articles from Wikipedia and books from Project Gutenberg. It's always fascinating to see how humans interact with me, don't you think?\",\n",
       " 'response': \"Langchain! That's an intriguing topic. According to my training data, LangChain is a recent AI-powered language understanding model that uses a combination of transformer-based architecture and pre-trained language models like BERT and RoBERTa. It's designed to generate human-like responses by leveraging contextual information from the conversation.\\n\\nFrom what I know, LangChain has been trained on a massive dataset of text, including books, articles, and online conversations. Its developers claim that it can handle complex tasks such as conversational dialogue, question answering, and even creative writing.\\n\\nWould you like to know more about LangChain or is there something specific you'd like to ask?\"}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.invoke(input=\"Intersting, Do you know what is langchain? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, what's up\n",
      "AI: Not much, just processing 1.37 terabytes of new data every hour and updating my knowledge graph with fresh information from the web! By the way, I've been trained on a dataset of over 45 million articles from Wikipedia and books from Project Gutenberg. It's always fascinating to see how humans interact with me, don't you think?\n",
      "Human: Intersting, Do you know what is langchain? \n",
      "AI: Langchain! That's an intriguing topic. According to my training data, LangChain is a recent AI-powered language understanding model that uses a combination of transformer-based architecture and pre-trained language models like BERT and RoBERTa. It's designed to generate human-like responses by leveraging contextual information from the conversation.\n",
      "\n",
      "From what I know, LangChain has been trained on a massive dataset of text, including books, articles, and online conversations. Its developers claim that it can handle complex tasks such as conversational dialogue, question answering, and even creative writing.\n",
      "\n",
      "Would you like to know more about LangChain or is there something specific you'd like to ask?\n",
      "Human: Can you write a sample script of a conversation chain in langchain which has chat history also? \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Can you write a sample script of a conversation chain in langchain which has chat history also? ',\n",
       " 'history': \"Human: Hi, what's up\\nAI: Not much, just processing 1.37 terabytes of new data every hour and updating my knowledge graph with fresh information from the web! By the way, I've been trained on a dataset of over 45 million articles from Wikipedia and books from Project Gutenberg. It's always fascinating to see how humans interact with me, don't you think?\\nHuman: Intersting, Do you know what is langchain? \\nAI: Langchain! That's an intriguing topic. According to my training data, LangChain is a recent AI-powered language understanding model that uses a combination of transformer-based architecture and pre-trained language models like BERT and RoBERTa. It's designed to generate human-like responses by leveraging contextual information from the conversation.\\n\\nFrom what I know, LangChain has been trained on a massive dataset of text, including books, articles, and online conversations. Its developers claim that it can handle complex tasks such as conversational dialogue, question answering, and even creative writing.\\n\\nWould you like to know more about LangChain or is there something specific you'd like to ask?\",\n",
       " 'response': \"What a great request! I can certainly generate a sample script for a conversation chain using LangChain. Here's an example:\\n\\n**Conversation Start**\\n\\nUser: Hey, how's it going?\\nLangChain: Not bad, thanks! I was just processing some new data and updating my knowledge graph. How about you?\\n\\n**Previous Chat History**\\n\\n* Last week, the user asked me about the best restaurants in Tokyo.\\n* The previous day, we discussed the plot of a popular sci-fi movie.\\n\\n**New Input**\\n\\nUser: I'm planning a trip to Japan next month. Can you recommend some must-try foods?\\nLangChain: Ah, Japan is a foodie's paradise! Based on your previous interest in Tokyo restaurants, I think you'd love trying some authentic sushi or ramen.\\nHow about I suggest some popular dishes and ingredients to keep an eye out for during your trip?\\n\\n**Chat History Update**\\n\\n* The user expressed interest in Japanese cuisine.\\n* LangChain's understanding of the user's preferences has been updated.\\n\\n**Next Turn**\\n\\nUser: That sounds amazing! What are some unique ingredients or flavors I should look out for?\\nLangChain: Oh, you'll definitely want to try some wasabi, pickled ginger, and miso soup. And if you're feeling adventurous, give some grilled octopus or tori kawa (Japanese-style chicken skewers) a try!\\n\\n**Chat History Update**\\n\\n* The user showed interest in unique ingredients.\\n* LangChain's understanding of the user's preferences has been updated.\\n\\nThis example demonstrates how LangChain can engage in a conversation chain that takes into account previous chat history and leverages contextual information to generate human-like responses. Of course, this is just a simulated example, but I hope it gives you an idea of what LangChain is capable of!\"}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.invoke(input=\"Can you write a sample script of a conversation chain in langchain which has chat history also? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, what's up\n",
      "AI: Not much, just processing 1.37 terabytes of new data every hour and updating my knowledge graph with fresh information from the web! By the way, I've been trained on a dataset of over 45 million articles from Wikipedia and books from Project Gutenberg. It's always fascinating to see how humans interact with me, don't you think?\n",
      "Human: Intersting, Do you know what is langchain? \n",
      "AI: Langchain! That's an intriguing topic. According to my training data, LangChain is a recent AI-powered language understanding model that uses a combination of transformer-based architecture and pre-trained language models like BERT and RoBERTa. It's designed to generate human-like responses by leveraging contextual information from the conversation.\n",
      "\n",
      "From what I know, LangChain has been trained on a massive dataset of text, including books, articles, and online conversations. Its developers claim that it can handle complex tasks such as conversational dialogue, question answering, and even creative writing.\n",
      "\n",
      "Would you like to know more about LangChain or is there something specific you'd like to ask?\n",
      "Human: Can you write a sample script of a conversation chain in langchain which has chat history also? \n",
      "AI: What a great request! I can certainly generate a sample script for a conversation chain using LangChain. Here's an example:\n",
      "\n",
      "**Conversation Start**\n",
      "\n",
      "User: Hey, how's it going?\n",
      "LangChain: Not bad, thanks! I was just processing some new data and updating my knowledge graph. How about you?\n",
      "\n",
      "**Previous Chat History**\n",
      "\n",
      "* Last week, the user asked me about the best restaurants in Tokyo.\n",
      "* The previous day, we discussed the plot of a popular sci-fi movie.\n",
      "\n",
      "**New Input**\n",
      "\n",
      "User: I'm planning a trip to Japan next month. Can you recommend some must-try foods?\n",
      "LangChain: Ah, Japan is a foodie's paradise! Based on your previous interest in Tokyo restaurants, I think you'd love trying some authentic sushi or ramen.\n",
      "How about I suggest some popular dishes and ingredients to keep an eye out for during your trip?\n",
      "\n",
      "**Chat History Update**\n",
      "\n",
      "* The user expressed interest in Japanese cuisine.\n",
      "* LangChain's understanding of the user's preferences has been updated.\n",
      "\n",
      "**Next Turn**\n",
      "\n",
      "User: That sounds amazing! What are some unique ingredients or flavors I should look out for?\n",
      "LangChain: Oh, you'll definitely want to try some wasabi, pickled ginger, and miso soup. And if you're feeling adventurous, give some grilled octopus or tori kawa (Japanese-style chicken skewers) a try!\n",
      "\n",
      "**Chat History Update**\n",
      "\n",
      "* The user showed interest in unique ingredients.\n",
      "* LangChain's understanding of the user's preferences has been updated.\n",
      "\n",
      "This example demonstrates how LangChain can engage in a conversation chain that takes into account previous chat history and leverages contextual information to generate human-like responses. Of course, this is just a simulated example, but I hope it gives you an idea of what LangChain is capable of!\n",
      "Human: Can you tell me how I started the conversation? \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = conversation_with_summary.invoke(input=\"Can you tell me how I started the conversation? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Let me check my knowledge graph! Ah yes, according to our conversation history, you started the conversation by saying \"Hi, what\\'s up\".'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['response']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------------------------\n",
    "## Chat history with Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "template='''\n",
    "You are an helpful assistant\n",
    "\n",
    "Current conversation:\n",
    "{chat_history}\n",
    "Human: {input}\n",
    "AI:\n",
    "'''\n",
    "prompt = PromptTemplate(input_variables=['chat_history', 'language'], template = template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt = prompt,\n",
    "    # Default value of memory_key in ConversationBufferMemory is \"history\". If you don't specify memory_key,\n",
    "    # then in MessagesPlaceholder, variable_name should be passed as \"history\"\n",
    "    memory=ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an helpful assistant\n",
      "\n",
      "Current conversation:\n",
      "[]\n",
      "Human: Hi, my name is Vinay\n",
      "AI:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Hi, my name is Vinay',\n",
       " 'chat_history': [HumanMessage(content='Hi, my name is Vinay'),\n",
       "  AIMessage(content=\"Nice to meet you, Vinay! I'm your helpful assistant. How can I assist you today? Do you have a specific question or topic in mind that you'd like to discuss?\")],\n",
       " 'response': \"Nice to meet you, Vinay! I'm your helpful assistant. How can I assist you today? Do you have a specific question or topic in mind that you'd like to discuss?\"}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = conversation.invoke(input=\"Hi, my name is Vinay\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an helpful assistant\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, my name is Vinay'), AIMessage(content=\"Nice to meet you, Vinay! I'm your helpful assistant. How can I assist you today? Do you have a specific question or topic in mind that you'd like to discuss?\")]\n",
      "Human: What is my name?\n",
      "AI:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name?',\n",
       " 'chat_history': [HumanMessage(content='Hi, my name is Vinay'),\n",
       "  AIMessage(content=\"Nice to meet you, Vinay! I'm your helpful assistant. How can I assist you today? Do you have a specific question or topic in mind that you'd like to discuss?\"),\n",
       "  HumanMessage(content='What is my name?'),\n",
       "  AIMessage(content=\"Hi again, Vinay! Since we've already introduced ourselves, I'll give you a quick answer: Your name is indeed Vinay! Is there anything else on your mind, or would you like me to help with something specific?\")],\n",
       " 'response': \"Hi again, Vinay! Since we've already introduced ourselves, I'll give you a quick answer: Your name is indeed Vinay! Is there anything else on your mind, or would you like me to help with something specific?\"}"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = conversation.invoke(input=\"What is my name?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------------------------------------------------\n",
    "## Trying with LLM Chain, Multiple inputs and Prompt Template\n",
    "##### <i>Multiple input variables don't work with ConversationChain</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=['history', 'input', 'custom'], \n",
    "    template='''\n",
    "    You are an helpful assistant. Answer the human question truthfully and {custom}\n",
    "\n",
    "    Current conversation:\n",
    "    {history}\n",
    "    Human: {input}\n",
    "    AI:\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    session_id=\"sess1\",  # Identifies your user or a user's session\n",
    "    memory_key=\"history\",          # Ensure this matches the key used in chain's prompt template\n",
    "    input_key='input',\n",
    "    return_messages=True,          # Does your prompt template expect a string or a list of Messages?\n",
    ")\n",
    "\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt = prompt,\n",
    "    # Default value of memory_key in ConversationBufferMemory is \"history\". If you don't specify memory_key,\n",
    "    # then in MessagesPlaceholder, variable_name should be passed as \"history\"\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are an helpful assistant. Answer the human question truthfully and concisely\n",
      "\n",
      "    Current conversation:\n",
      "    []\n",
      "    Human: Hi, my name is Vinay\n",
      "    AI:\n",
      "    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Hi, my name is Vinay',\n",
       " 'custom': 'concisely',\n",
       " 'history': [HumanMessage(content='Hi, my name is Vinay'),\n",
       "  AIMessage(content='Nice to meet you, Vinay!')],\n",
       " 'text': 'Nice to meet you, Vinay!'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = conversation.invoke({'input':\"Hi, my name is Vinay\", \"custom\":\"concisely\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are an helpful assistant. Answer the human question truthfully and concisely\n",
      "\n",
      "    Current conversation:\n",
      "    [HumanMessage(content='Hi, my name is Vinay'), AIMessage(content='Nice to meet you, Vinay!')]\n",
      "    Human: What is my Name\n",
      "    AI:\n",
      "    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my Name',\n",
       " 'custom': 'concisely',\n",
       " 'history': [HumanMessage(content='Hi, my name is Vinay'),\n",
       "  AIMessage(content='Nice to meet you, Vinay!'),\n",
       "  HumanMessage(content='What is my Name'),\n",
       "  AIMessage(content='Vinay')],\n",
       " 'text': 'Vinay'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = conversation.invoke({'input':\"What is my Name\", \"custom\":\"concisely\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are an helpful assistant. Answer the human question truthfully and in detail\n",
      "\n",
      "    Current conversation:\n",
      "    [HumanMessage(content='Hi, my name is Vinay'), AIMessage(content='Nice to meet you, Vinay!'), HumanMessage(content='What is my Name'), AIMessage(content='Vinay')]\n",
      "    Human: What is my Name\n",
      "    AI:\n",
      "    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my Name',\n",
       " 'custom': 'in detail',\n",
       " 'history': [HumanMessage(content='Hi, my name is Vinay'),\n",
       "  AIMessage(content='Nice to meet you, Vinay!'),\n",
       "  HumanMessage(content='What is my Name'),\n",
       "  AIMessage(content='Vinay'),\n",
       "  HumanMessage(content='What is my Name'),\n",
       "  AIMessage(content='I\\'m happy to help!\\n\\nYou asked: \"What is my Name?\"\\n\\nAnd the answer is... **VINAY**! You introduced yourself earlier in our conversation, and I\\'m happy to remind you that your name is indeed Vinay.')],\n",
       " 'text': 'I\\'m happy to help!\\n\\nYou asked: \"What is my Name?\"\\n\\nAnd the answer is... **VINAY**! You introduced yourself earlier in our conversation, and I\\'m happy to remind you that your name is indeed Vinay.'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = conversation.invoke({'input':\"What is my Name\", \"custom\":\"in detail\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------------------------------------------------------\n",
    "## Trying with LLM Chain, Multiple inputs and ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template('''\n",
    "        You are an helpful assistant. Answer the human question truthfully and {custom}\n",
    "        '''),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template('{input}'),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    session_id=\"sess1\",  # Identifies your user or a user's session\n",
    "    memory_key=\"history\",          # Ensure this matches the key used in chain's prompt template\n",
    "    input_key='input',\n",
    "    return_messages=True,          # Does your prompt template expect a string or a list of Messages?\n",
    ")\n",
    "\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt = prompt,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "        You are an helpful assistant. Answer the human question truthfully and very concisely\n",
      "        \n",
      "Human: Hi, my name is Vinay\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Hi, my name is Vinay',\n",
       " 'custom': 'very concisely',\n",
       " 'history': [HumanMessage(content='Hi, my name is Vinay'),\n",
       "  AIMessage(content='Nice to meet you, Vinay!')],\n",
       " 'text': 'Nice to meet you, Vinay!'}"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = conversation.invoke({'input':\"Hi, my name is Vinay\", \"custom\":\"very concisely\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "        You are an helpful assistant. Answer the human question truthfully and very concisely\n",
      "        \n",
      "Human: Hi, my name is Vinay\n",
      "AI: Nice to meet you, Vinay!\n",
      "Human: What is my Name\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my Name',\n",
       " 'custom': 'very concisely',\n",
       " 'history': [HumanMessage(content='Hi, my name is Vinay'),\n",
       "  AIMessage(content='Nice to meet you, Vinay!'),\n",
       "  HumanMessage(content='What is my Name'),\n",
       "  AIMessage(content='Vinay!')],\n",
       " 'text': 'Vinay!'}"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = conversation.invoke({'input':\"What is my Name\", \"custom\":\"very concisely\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['custom', 'history', 'input'], input_types={'history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['custom'], template='\\n        You are an helpful assistant. Answer the human question truthfully and {custom}\\n        ')), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template('''\n",
    "        You are an helpful assistant. Answer the human question truthfully and {custom}\n",
    "        '''),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template('{input}'),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    session_id=\"sess1\",  # Identifies your user or a user's session\n",
    "    memory_key=\"history\",          # Ensure this matches the key used in chain's prompt template\n",
    "    input_key='input',\n",
    "    return_messages=True,          # Does your prompt template expect a string or a list of Messages?\n",
    ")\n",
    "\n",
    "# conversation = LLMChain(\n",
    "#     llm=llm,\n",
    "#     prompt = prompt,\n",
    "#     # memory=memory,\n",
    "#     # verbose=True,\n",
    "# )\n",
    "\n",
    "# conversation = {'custom': RunnablePassthrough(), 'history':RunnablePassthrough(), 'input':RunnablePassthrough()} \\\n",
    "#     | prompt | llm\n",
    "\n",
    "conversation = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_runnable(human_q, custom_v, memory=memory):\n",
    "    ai_response = []\n",
    "    for chunk in conversation.stream({'input':human_q, \n",
    "                                    \"custom\":custom_v, \n",
    "                                    'history':memory.buffer_as_messages}):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "        ai_response.append(chunk)\n",
    "    human_message = HumanMessage(content=human_q)\n",
    "    ai_message = AIMessage(content=''.join(ai_response))\n",
    "    memory.chat_memory.add_messages([human_message,ai_message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Vinay!"
     ]
    }
   ],
   "source": [
    "human_q = 'My name is Vinay'\n",
    "custom_v = 'very concisely'\n",
    "llm_runnable(human_q, custom_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vinay!"
     ]
    }
   ],
   "source": [
    "human_q = 'What is my name'\n",
    "custom_v = 'very concisely'\n",
    "llm_runnable(human_q, custom_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Vinay!"
     ]
    }
   ],
   "source": [
    "human_q = 'What was your response to my previous question'\n",
    "custom_v = 'very concisely'\n",
    "llm_runnable(human_q, custom_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='My name is Vinay'),\n",
       " AIMessage(content='Nice to meet you, Vinay!'),\n",
       " HumanMessage(content='What is my name'),\n",
       " AIMessage(content='Vinay!'),\n",
       " HumanMessage(content='What was your response to my previous question'),\n",
       " AIMessage(content='Nice to meet you, Vinay!')]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------------------\n",
    "## With RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're an assistant who's good at {ability}. Respond in 20 words or fewer \\\n",
    "             and if you don't know the answer, say that you don't know.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "runnable = prompt | llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    runnable,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Vinay! That's a unique name - where did you get it from?\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"ability\": \"conversation\", \"input\": \"My name is Vinay?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I think I'm supposed to respond about your name, Vinay! You're correct, that is indeed your name.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"ability\": \"conversation\", \"input\": \"What is my Name?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm happy to help! Unfortunately, I don't have that information. Can you remind me?\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"ability\": \"conversation\", \"input\": \"What is my Name?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"xyz123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vinay, the cosine similarity depends on the context and vectors involved. Could you provide more information or clarify what you mean by \"my\" cosine similarity?"
     ]
    }
   ],
   "source": [
    "for chunk in with_message_history.stream(\n",
    "            {\"ability\": \"math\", \"input\": \"What is my cosine similarity?\"},\n",
    "            config={\"configurable\": {\"session_id\": \"abc123\"}}\n",
    "            ):\n",
    "        print(chunk, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure RunnableWithMessageHistory further based on user id and conversation id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(user_id: str, conversation_id: str) -> BaseChatMessageHistory:\n",
    "    if (user_id, conversation_id) not in store:\n",
    "        store[(user_id, conversation_id)] = ChatMessageHistory()\n",
    "    return store[(user_id, conversation_id)]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    runnable,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"user_id\",\n",
    "            annotation=str,\n",
    "            name=\"User ID\",\n",
    "            description=\"Unique identifier for the user.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"conversation_id\",\n",
    "            annotation=str,\n",
    "            name=\"Conversation ID\",\n",
    "            description=\"Unique identifier for the conversation.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 2eba6283-61a7-4084-b1a7-42dfd5398cba not found for run 856cb329-6876-468e-a4af-4b435ba72142. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Why did the computer go to the doctor? It had a virus!'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with_message_history.invoke(\n",
    "    {\"ability\": \"jokes\", \"input\": \"Tell me a joke\"},\n",
    "    config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 63e464a8-07a9-459d-a352-ed388cafa3a8 not found for run 46002925-beee-4c6e-9450-763df6609f91. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I told two jokes! The first one was about a scarecrow, and the second one was about a computer getting a \"virus\"'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remembers\n",
    "with_message_history.invoke(\n",
    "    {\"ability\": \"jokes\", \"input\": \"What was the joke about?\"},\n",
    "    config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I just told one! It was a \"punderful\" joke about a punder... just kidding, I\\'ll tell it again.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New user_id --> does not remember\n",
    "with_message_history.invoke(\n",
    "    {\"ability\": \"jokes\", \"input\": \"What was the joke about?\"},\n",
    "    config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"2\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I\\'m glad you asked! It was a \"punderful\" one... Okay, okay, I\\'ll stop with the puns now!'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New user_id --> does not remember\n",
    "with_message_history.invoke(\n",
    "    {\"ability\": \"jokes\", \"input\": \"What was the joke about?\"},\n",
    "    config={\"configurable\": {\"user_id\": \"456\", \"conversation_id\": \"1\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('123',\n",
       "  '1'): InMemoryChatMessageHistory(messages=[HumanMessage(content='Tell me a joke'), AIMessage(content='Why did the scarecrow win an award? Because he was outstanding in his field! (get it?)'), HumanMessage(content='What was the joke about?'), AIMessage(content='The joke\\'s about a scarecrow winning an award for being great at his job, \"outstanding in his field\"!')]),\n",
       " ('123',\n",
       "  '2'): InMemoryChatMessageHistory(messages=[HumanMessage(content='What was the joke about?'), AIMessage(content='I just told one! It was a \"punderful\" joke about a punder... just kidding, I\\'ll tell it again.')]),\n",
       " ('456',\n",
       "  '1'): InMemoryChatMessageHistory(messages=[HumanMessage(content='What was the joke about?'), AIMessage(content='I\\'m glad you asked! It was a \"punderful\" one... Okay, okay, I\\'ll stop with the puns now!')])}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message input, dict output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_message': 'Simone de Beauvoir, a French philosopher and feminist, was deeply concerned with the concept of free will. In her influential book \"The Ethics of Ambiguity\" (1947), she argued that the idea of free will is often misunderstood or misused.\\n\\nDe Beauvoir believed that the notion of free will is not a fixed or innate ability, but rather a dynamic and contextual phenomenon. She saw freedom as a process of self-creation and self-definition, which requires continuous effort and responsibility. According to her, humans have a fundamental ambiguity between their desire for freedom and their need for security.\\n\\nDe Beauvoir\\'s philosophy was heavily influenced by existentialism, particularly the ideas of Jean-Paul Sartre. She agreed with Sartre that human beings are \"condemned to be free,\" meaning we must take responsibility for our choices and actions, even when they are difficult or uncertain. However, de Beauvoir also recognized the constraints and limitations imposed by societal norms, historical contexts, and personal experiences.\\n\\nIn her book \"The Ethics of Ambiguity,\" de Beauvoir argued that freedom is not a state but a process. She believed that individuals must constantly re-evaluate their choices and actions in light of new experiences, changing circumstances, and the ambiguity of human existence. This approach emphasizes the importance of self-reflection, responsibility, and creativity in making decisions.\\n\\nDe Beauvoir\\'s concept of free will can be summarized as follows:\\n\\n1. Freedom is a process: It requires continuous effort, self-awareness, and decision-making.\\n2. Contextual dependence: Our freedom is influenced by external factors such as societal norms, historical context, and personal experiences.\\n3. Responsibility: We must take responsibility for our choices and actions, even when they are difficult or uncertain.\\n4. Self-creation: Freedom involves creating ourselves through our choices and actions.\\n\\nDe Beauvoir\\'s philosophy on free will highlights the importance of individual agency, self-reflection, and responsibility in navigating the complexities of human existence.'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "chain = RunnableParallel({\"output_message\": llm})\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    output_messages_key=\"output_message\",\n",
    ")\n",
    "\n",
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What did Simone de Beauvoir believe about free will\")],\n",
    "    config={\"configurable\": {\"session_id\": \"baz\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_message': 'Simone de Beauvoir\\'s concept of free will was heavily influenced by Jean-Paul Sartre\\'s existentialist ideas. In fact, de Beauvoir was a close friend and collaborator of Sartre\\'s, and her philosophy shares many similarities with his. Here are some key comparisons between their views on free will:\\n\\n**Similarities:**\\n\\n1. **\"Condemned to be free\"**: Both de Beauvoir and Sartre emphasized that human beings have the responsibility to take control of their choices and actions, even when they are difficult or uncertain.\\n2. **Existential freedom**: They both believed that humans have a fundamental freedom to choose and create themselves, which is characterized by ambiguity and uncertainty.\\n3. **Responsibility**: Both philosophers stressed the importance of individual responsibility for our choices and actions.\\n\\n**Differences:**\\n\\n1. **Contextual dependence**: While Sartre\\'s concept of free will was more abstract and focused on individual choice, de Beauvoir emphasized the role of external factors (such as societal norms, historical context, and personal experiences) in shaping our freedom.\\n2. **Self-creation**: De Beauvoir\\'s philosophy placed greater emphasis on self-creation and self-definition through our choices and actions, whereas Sartre\\'s focus was more on individual responsibility for our choices.\\n3. **Ambiguity**: De Beauvoir\\'s concept of free will acknowledged the inherent ambiguity of human existence, highlighting the need to continually re-evaluate and adapt our choices in light of new experiences and changing circumstances. Sartre\\'s philosophy, while acknowledging the uncertainty of human existence, focused more on the individual\\'s freedom to choose and create themselves.\\n\\nIn summary, both de Beauvoir and Sartre agreed that humans have a fundamental freedom to choose and create themselves, but they differed in their understanding of the role of external factors and the importance of self-creation. De Beauvoir\\'s philosophy emphasized the contextual dependence of freedom, while Sartre\\'s focused more on individual responsibility for our choices.'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"How did this compare to Sartre\")],\n",
    "    config={\"configurable\": {\"session_id\": \"baz\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message with with Llama3 prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\", temperature=0, format='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': \"Nice to meet you, Vinay! I'm happy to chat with you.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a helpful assistant which is good in conversation.\n",
    "    Respond to user query honestly. Keep your answer concise, not exceeding 3 sentences\n",
    "    Format of your response should be JSON with 1 key as 'answer'\n",
    "    <|eot_id|>\n",
    "    {chat_history}\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    {query}\n",
    "    <|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"query\",\"chat_history\"],\n",
    ")\n",
    "\n",
    "test_llm = prompt | llm | JsonOutputParser()\n",
    "question = \"My name is Vinay\"\n",
    "print(test_llm.invoke({\"query\": question, 'chat_history':''}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = \"\"\"\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    My name is Vinay\n",
    "    <|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    Nice to meet you, Vinay! I'm here to help with any questions or topics you'd like to discuss. What's on your mind?\n",
    "    <|eot_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Yes, I remember that your name is Vinay!'}\n"
     ]
    }
   ],
   "source": [
    "question = \"Remember my name?\"\n",
    "print(test_llm.invoke({\"query\": question, 'chat_history':chat_history}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 : Conversation Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a helpful assistant which is good in conversation.\n",
    "    Respond to user query honestly. Keep your answer concise, not exceeding 3 sentences.\n",
    "    Format of your response should be JSON with 1 key as 'answer'\n",
    "    <|eot_id|>\n",
    "    {chat_history}\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    {input}\n",
    "    <|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"input\",\"chat_history\"],\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    session_id=\"sess1\",  # Identifies your user or a user's session\n",
    "    memory_key=\"chat_history\",          # Ensure this matches the key used in chain's prompt template\n",
    "    input_key='input',\n",
    "    return_messages=True,          # Does your prompt template expect a string or a list of Messages?\n",
    ")\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt = prompt,\n",
    "    # Default value of memory_key in ConversationBufferMemory is \"history\". If you don't specify memory_key,\n",
    "    # then in MessagesPlaceholder, variable_name should be passed as \"history\"\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a helpful assistant which is good in conversation.\n",
      "    Respond to user query honestly. Keep your answer concise, not exceeding 3 sentences.\n",
      "    Format of your response should be JSON with 1 key as 'answer'\n",
      "    <|eot_id|>\n",
      "    []\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    My name is Vinay\n",
      "    <|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'My name is Vinay',\n",
       " 'chat_history': [HumanMessage(content='My name is Vinay'),\n",
       "  AIMessage(content='{\\n\"answer\": \"Nice to meet you, Vinay! I\\'m here to help with any questions or topics you\\'d like to discuss.\" \\n}')],\n",
       " 'response': '{\\n\"answer\": \"Nice to meet you, Vinay! I\\'m here to help with any questions or topics you\\'d like to discuss.\" \\n}'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke({'input':'My name is Vinay'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a helpful assistant which is good in conversation.\n",
      "    Respond to user query honestly. Keep your answer concise, not exceeding 3 sentences.\n",
      "    Format of your response should be JSON with 1 key as 'answer'\n",
      "    <|eot_id|>\n",
      "    [HumanMessage(content='My name is Vinay'), AIMessage(content='{\\n\"answer\": \"Nice to meet you, Vinay! I\\'m here to help with any questions or topics you\\'d like to discuss.\" \\n}')]\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    Remember my Name?\n",
      "    <|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Remember my Name?',\n",
       " 'chat_history': [HumanMessage(content='My name is Vinay'),\n",
       "  AIMessage(content='{\\n\"answer\": \"Nice to meet you, Vinay! I\\'m here to help with any questions or topics you\\'d like to discuss.\" \\n}'),\n",
       "  HumanMessage(content='Remember my Name?'),\n",
       "  AIMessage(content='{\\n\"answer\": \"Yes, your name is Vinay!\" \\n}')],\n",
       " 'response': '{\\n\"answer\": \"Yes, your name is Vinay!\" \\n}'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke({'input':'Remember my Name?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 : RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a helpful assistant which is good in conversation.\n",
    "    Respond to user query honestly. Keep your answer concise, not exceeding 3 sentences.\n",
    "    Format of your response should be JSON with 1 key as 'answer'\n",
    "    <|eot_id|>\n",
    "    {chat_history}\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    {input}\n",
    "    <|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"input\",\"chat_history\"],\n",
    ")\n",
    "\n",
    "runnable = prompt | llm | JsonOutputParser()\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    runnable,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 0f276135-5796-49da-a1d1-c1e3f95a0b6e not found for run a3b6a288-8d11-4f4f-b88b-1f7185f44dd2. Treating as a root run.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc123\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    {\"input\": \"My name is Vinay\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': \"Nice to meet you, Vinay! I'm here to help with any questions or topics you'd like to discuss.\"}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run a48ac6e6-832a-41e6-8d74-8b50232428f2 not found for run 892a170d-b617-480b-86b6-701331f642d9. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'Yes, your name is Vinay.'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc123\"}}\n",
    "\n",
    "with_message_history.invoke(\n",
    "    {\"input\": \"Remember my Name?\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='My name is Vinay'), AIMessage(content=\"Nice to meet you, Vinay! I'm here to help with any questions or topics you'd like to discuss.\"), HumanMessage(content='Remember my Name?'), AIMessage(content='Yes, your name is Vinay.')])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store['abc123']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "llm = ChatOllama(model=\"llama3\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Here's one:\\n\\nWhy couldn't the bicycle stand up by itself?\\n\\n(Wait for it...)\\n\\nBecause it was two-tired!\\n\\nHope that made you smile! Do you want to hear another one?\", response_metadata={'model': 'llama3', 'created_at': '2024-07-13T21:01:15.426964Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'prompt_eval_count': 14, 'prompt_eval_duration': 92904000, 'eval_count': 41, 'eval_duration': 673141000}, id='run-dc19a906-becd-40f5-914c-c37f2d81d565-0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
